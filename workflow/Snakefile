import os
import json
from snakemake.utils import min_version
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

min_version("6.6.1")

configfile: 
    "config/config.yaml"

with open("config/samples.tsv") as sample_file:
    h = sample_file.readline().rstrip('\n').split('\t')
    exp_ind = h.index("Experiment")
    rep_ind = h.index("Replicate")
    mod_ind = h.index("Modality")
    gen_ind = h.index("Genome")
    sample_config = {}
    samples = []
    for line in sample_file:
        if line.startswith("#"):
            continue
        if line.startswith("@"):
            break
        if line.startswith("$"):
            sample_config.clear()
            samples.clear()
            continue
        entries = line.rstrip('\n').split('\t')
        exp = entries[exp_ind]
        rep = int(entries[rep_ind])
        mod = entries[mod_ind]
        gen = entries[gen_ind]
        sample_id = f"{exp}-{rep}"
        sample_config[sample_id] = {
            "experiment": exp,
            "replicate": rep,
            "modality": mod,
            "genome": gen
        }
        samples.append(sample_id)

workdir: 
    config['workdir']

envvars:
    "DCC_API_KEY",
    "DCC_SECRET_KEY"

HTTP = HTTPRemoteProvider()

max_threads = config["max_threads_per_rule"]

def script_path(script_name):
    return str(workflow.source_path(script_name))

include:
    "rules/fastqs.smk"
include:
    "rules/mapping.smk"
include:
    "rules/filtering.smk"
include:
    "rules/fragments.smk"
include:
    "rules/analyses.smk"
include:
    "rules/metadata.smk"
include:
    "rules/submit.smk"

localrules: 
    all, 
    fragments,
    build_metadata, 
    fetch_inputs,
    submit, 
    query_portal,
    fetch_whitelist,
    move_whitelist,
    fetch_index,
    decompress_index,
    extract_index,
    idx_prefix_placeholder,
    move_index_files,
    fetch_blacklist,
    fetch_bsgenome,
    fetch_gene_anno,
    fetch_peak_anno,
    move_archr_files,
    build_bulk_caper_config

ruleorder: extract_index > idx_prefix_placeholder

rule all:
    """
    Generate all outputs (default)
    """
    input: 
        expand("results/{sample}/fastqs/R1_trim.fastq.gz", sample=samples),
        expand("results/{sample}/fastqs/R2_trim.fastq.gz", sample=samples),
        expand("results/{sample}/fastqs/barcode_revcomp.txt", sample=samples),
        expand("results/{sample}/fastqs/barcode_matching.tsv", sample=samples), 
        expand("results/{sample}/fastqs/trim_adapters.txt", sample=samples),
        expand("results/{sample}/mapping/raw.bam", sample=samples),
        expand("results/{sample}/mapping/raw_collated.bam", sample=samples),
        expand("results/{sample}/mapping/samstats_raw.txt", sample=samples),
        expand("results/{sample}/filtering/filtered.bam", sample=samples),
        expand("results/{sample}/filtering/filtered.bam.bai", sample=samples),
        expand("results/{sample}/filtering/frac_mito.tsv", sample=samples),
        expand("results/{sample}/filtering/markdup.txt", sample=samples),
        expand("results/{sample}/filtering/pbc_stats.tsv", sample=samples),
        expand("results/{sample}/filtering/samstats_filtered.txt", sample=samples),
        expand("results/{sample}/fragments/fragments.tsv.gz", sample=samples),
        expand("results/{sample}/fragments/fragments.tsv.gz.tbi", sample=samples),
        expand("results/{sample}/fragments/fragments.tar.gz", sample=samples),
        expand("results/{sample}/fragments/barcode_pairs_multiplets.tsv", sample=samples),
        expand("results/{sample}/fragments/barcode_pairs_expanded.tsv.gz", sample=samples),
        expand("results/{sample}/fragments/multiplet_barcodes_status.tsv", sample=samples),
        expand("results/{sample}/fragments/multiplets_threshold_plot.png", sample=samples),
        expand("results/{sample}/fragments/multiplet_stats.txt", sample=samples),
        expand("results/{sample}/analyses/archr_project.tar.gz", sample=samples),
        expand("results/{sample}/analyses/archr_doublet_summary.pdf", sample=samples),
        expand("results/{sample}/analyses/archr_doublet_summary.tsv", sample=samples),
        expand("results/{sample}/analyses/archr_fragment_size_distribution.pdf", sample=samples),
        expand("results/{sample}/analyses/archr_pre_filter_metadata.tsv", sample=samples),
        expand("results/{sample}/analyses/archr_tss_by_unique_frags.pdf", sample=samples)

rule fragments:
    """
    Generate all outputs up to fragment files
    """
    input: 
        expand("results/{sample}/fastqs/R1_trim.fastq.gz", sample=samples),
        expand("results/{sample}/fastqs/R2_trim.fastq.gz", sample=samples),
        expand("results/{sample}/fastqs/barcode_revcomp.txt", sample=samples),
        expand("results/{sample}/fastqs/barcode_matching.tsv", sample=samples), 
        expand("results/{sample}/fastqs/trim_adapters.txt", sample=samples),
        expand("results/{sample}/mapping/raw.bam", sample=samples),
        expand("results/{sample}/mapping/raw_collated.bam", sample=samples),
        expand("results/{sample}/mapping/samstats_raw.txt", sample=samples),
        expand("results/{sample}/filtering/filtered.bam", sample=samples),
        expand("results/{sample}/filtering/filtered.bam.bai", sample=samples),
        expand("results/{sample}/filtering/frac_mito.tsv", sample=samples),
        expand("results/{sample}/filtering/markdup.txt", sample=samples),
        expand("results/{sample}/filtering/pbc_stats.tsv", sample=samples),
        expand("results/{sample}/filtering/samstats_filtered.txt", sample=samples),
        expand("results/{sample}/fragments/fragments.tsv.gz", sample=samples),
        expand("results/{sample}/fragments/fragments.tsv.gz.tbi", sample=samples),
        expand("results/{sample}/fragments/fragments.tar.gz", sample=samples),
        expand("results/{sample}/fragments/barcode_pairs_multiplets.tsv", sample=samples),
        expand("results/{sample}/fragments/barcode_pairs_expanded.tsv.gz", sample=samples),
        expand("results/{sample}/fragments/multiplet_barcodes_status.tsv", sample=samples),
        expand("results/{sample}/fragments/multiplets_threshold_plot.png", sample=samples),
        expand("results/{sample}/fragments/multiplet_stats.txt", sample=samples)

rule fetch_inputs:
    """
    Fetch input metadata from portal
    """
    input: 
        expand("results/{sample}/input_data.json", sample=samples)

rule build_metadata:
    """
    Build file and QC metadata for results
    """
    input: 
        "metadata_collate/R1_trim_metadata_all.tsv",
        "metadata_collate/R2_trim_metadata_all.tsv",
        "metadata_collate/reads_qc_metadata_all.tsv",
        "metadata_collate/raw_bam_metadata_all.tsv",
        "metadata_collate/alignments_raw_qc_metadata_all.tsv",
        "metadata_collate/filtered_bam_metadata_all.tsv",
        "metadata_collate/alignments_filtered_qc_metadata_all.tsv",
        "metadata_collate/alignments_lib_comp_qc_metadata_all.tsv",
        "metadata_collate/fragments_metadata_all.tsv",
        "metadata_collate/fragments_qc_metadata_all.tsv",
        "metadata_collate/analyses_metadata_all.tsv",
        "metadata_collate/analyses_qc_metadata_all.tsv",
        "metadata_collate/summary_qc_metadata_all.tsv"

rule submit:
    """
    Submit outputs to ENCODE portal
    """
    input: 
        expand("submit/{sample}/submit.done", sample=samples)

checkpoint query_portal:
    """
    Query ENCODE portal by experiment accession and replicate
    """
    output:
        "results/{sample}/input_data.json"
    params:
        experiment = lambda w: sample_config[w.sample]["experiment"],
        replicate = lambda w: sample_config[w.sample]["replicate"],
        modality = lambda w: sample_config[w.sample]["modality"],
        assembly = lambda w: sample_config[w.sample]["genome"],
        dcc_api_key = os.environ["DCC_API_KEY"], 
        dcc_secret_key = os.environ["DCC_SECRET_KEY"]
    log:
        directory("logs/{sample}/query")
    conda:
        "envs/portal.yaml"
    script:
        "scripts/encode_query.py"

class SampleDataFetcher:
    """
    Data structure for caching queried input data
    """
    def __init__(self, rule):
        self.rule = rule
        self.cache = {}

    def __getitem__(self, sample):
        if sample in self.cache:
            return self.cache[sample]
        with self.rule.get(sample=sample).output[0].open() as f:
            data = json.load(f)
        self.cache[sample] = data
        return data

sample_data = SampleDataFetcher(checkpoints.query_portal) 

rule fetch_whitelist:
    """
    Fetch barcode whitelist
    """
    input:
        lambda w: HTTP.remote(config["bc_whitelist"][w.modality], insecure=(not config["bc_https"]))
    output:
        "bc_whitelists/{modality}.txt.gz"
    conda:
        "envs/portal.yaml"
    shell:
        "mv {input} {output}"

rule move_whitelist:
    """
    Move barcode whitelist files to final location
    """
    input:
        lambda w: [f"bc_whitelists/{k}.txt.gz" for k in config["bc_whitelist"].keys()]

rule fetch_index:
    """
    Fetch Bowtie2 index archive
    """
    output:
        "bwt2_idx/zipped/{genome_name}.tar.gz"
    params:
        url = lambda w: config["genome"][w.genome_name]["bwt2_idx"]
    conda:
        "envs/portal.yaml"
    shell:
        "curl --no-progress-meter -L {params.url} > {output}"

rule decompress_index:
    """
    Decompress Bowtie2 index archive
    """
    input:
        "bwt2_idx/zipped/{genome_name}.tar.gz"
    output:
        "bwt2_idx/zipped/{genome_name}.tar"
    conda:
        "envs/portal.yaml"
    shell:
        "gunzip -c {input} > {output}"

rule extract_index:
    """
    Extract index files from tarball
    """
    input:
        "bwt2_idx/zipped/{genome_name}.tar"
    output:
        "bwt2_idx/unpacked/{genome_name}/{bwt2_prefix}.{file}.bt2"
    params:
        ind = lambda w: f"{w.bwt2_prefix}.{w.file}.bt2"
    conda:
        "envs/portal.yaml"
    shell:
        "tar -xOf {input} '{params.ind}' > {output}"

rule idx_prefix_placeholder:
    """
    Create Bowtie2 index prefix dummy file
    """
    output:
        touch("bwt2_idx/unpacked/{genome_name}/{bwt2_prefix}")

def get_idx_files_all(w):
    return [
        os.path.join("bwt2_idx", "unpacked", k, f"{v['bwt2_idx_prefix']}.{s}.bt2") 
        for s in ["1", "2", "3", "4", "rev.1", "rev.2"]
        for k, v in config["genome"].items()
    ]

rule move_index_files:
    """
    Move index files to final location
    """
    input:
        get_idx_files_all

rule fetch_blacklist:
    """
    Fetch genome blacklist
    """
    output:
        "archr_genome/{genome_name}/blacklist.bed"
    params:
        url = lambda w: config["genome"][w.genome_name]["blacklist"]
    conda:
        "envs/portal.yaml"
    shell:
        "curl --no-progress-meter -L {params.url} | zcat | cat - <(echo chrX$'\\t'0$'\\t'1) | sort -k1,1V -k2,2n -k3,3n > {output}" 

rule fetch_bsgenome:
    """
    Fetch BSGenome R packages
    """
    output:
        "archr_genome/{genome_name}/{bsgenome}.tar.gz"
    params:
        url = lambda w: config["genome"][w.genome_name]["bsgenome"]
    conda:
        "envs/portal.yaml"
    shell:
        "curl --no-progress-meter -L {params.url} > {output}" 

rule fetch_gene_anno:
    """
    Fetch ArchR gene annotations
    """
    output:
        "archr_genome/{genome_name}/{gene_anno}.rda"
    params:
        url = lambda w: config["genome"][w.genome_name]["archr_gene_anno"]
    conda:
        "envs/portal.yaml"
    shell:
        "curl --no-progress-meter -L {params.url} > {output}" 

rule fetch_peak_anno:
    """
    Fetch ArchR peak annotations
    """
    output:
        "archr_genome/{genome_name}/{peak_anno}.Anno"
    params:
        url = lambda w: config["genome"][w.genome_name]["archr_peak_anno"]
    conda:
        "envs/portal.yaml"
    shell:
        "curl --no-progress-meter -L {params.url} > {output}" 

def get_archr_files_all(w):
    p = []
    for k in config["genome"].keys():
        p.extend([
            f"archr_genome/{k}/{k}_blacklist.bed",
            f"archr_genome/{k}/{config['genome'][k]['bsgenome_name']}.tar.gz",
            f"archr_genome/{k}/{config['genome'][k]['gene_anno_name']}.rda",
            f"archr_genome/{k}/{config['genome'][k]['peak_anno_name']}.Anno"
        ])
    return p

rule move_archr_files:
    """
    Move ArchR data files to final location
    """
    input:
        get_archr_files_all

rule resubmit_files:
    """
    Resubmit files that failed to upload
    """
    input:
        ENCFF611IWH = "results/ENCSR481YWC-1/fastqs/R2_trim.fastq.gz",
        ENCFF863MSU = "results/ENCSR639HAJ-1/mapping/raw.bam",
        ENCFF401TTS = "results/ENCSR290IUT-1/mapping/raw.bam",
        ENCFF871AGQ = "results/ENCSR568XID-1/fragments/fragments.tar.gz",
        ENCFF288DCD = "results/ENCSR726QTF-1/mapping/raw.bam",
        ENCFF863UAO = "results/ENCSR534KYJ-1/mapping/raw.bam",
        ENCFF371RGF = "results/ENCSR341TTX-1/fastqs/R2_trim.fastq.gz"
    output:
        "reupload_touch.txt"
    params:
        dcc_api_key = os.environ["DCC_API_KEY"], 
        dcc_secret_key = os.environ["DCC_SECRET_KEY"]
    log:
        directory("logs/{sample}/query")
    conda:
        "envs/portal.yaml"
    script:
        "scripts/encode_resubmit.py"

# rule build_bulk_caper_config:
#     """
#     Build input json for bulk pipeline integration
#     """
#     input:
#         ancient("results/{sample}/input_data.json")
#     output:
#         expand("bulk_configs/{sample}.json", sample=samples)
#     params:
#         sample = lambda w: w.sample,
#         bucket = config["bucket"],
#         genome = lambda w: sample_data[w.sample]["genome"],
#         read_length = lambda w: sample_data[w.sample]["read_length"]
#     conda:
#         "envs/portal.yaml"
#     script:
#         "scripts/build_bulk_caper_config.py" 
